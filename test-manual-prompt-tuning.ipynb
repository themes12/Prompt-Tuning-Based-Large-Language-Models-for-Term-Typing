{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 11 15:49:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:1A:00.0 Off |                  Off |\n",
      "| 69%   86C    P2             296W / 300W |  44505MiB / 49140MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               Off | 00000000:68:00.0 Off |                  Off |\n",
      "| 71%   86C    P2             297W / 300W |  43652MiB / 49140MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1104      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    0   N/A  N/A     81779      C   python3                                   44490MiB |\n",
      "|    1   N/A  N/A      1104      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    1   N/A  N/A      1316      G   /usr/bin/gnome-shell                          5MiB |\n",
      "|    1   N/A  N/A     83983      C   python3                                   43624MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install git+https://github.com/huggingface/peft transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftModel, PeftConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "peft_model_id = \"./prompt_tuning/wn18rr/PT/template-8/bloom_1b7\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "#print(model)\n",
    "final_prompt_embeddings = model.prompt_encoder[\"default\"].embedding.weight.detach().clone()\n",
    "#device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    f\"Perform a sentence completion on the following sentence: seashore. 'seashore' part of speech is a ___.\\nThe answer is\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "#model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=15,\n",
    "    prompt_tuning_init_text=\"Classify part of speech of given word in the sentence to noun, verb, adjective or adverb\",\n",
    "    tokenizer_name_or_path=\"bigscience/bloom-1b7\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-1b7\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "initial_prompt_embeddings = model.prompt_encoder[\"default\"].embedding.weight.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0035, -0.0311, -0.0123,  ..., -0.0041,  0.0064,  0.0044],\n",
      "        [ 0.0231, -0.0135,  0.0042,  ..., -0.0420,  0.0073, -0.0134],\n",
      "        [-0.0067,  0.0186,  0.0099,  ...,  0.0083,  0.0137,  0.0059],\n",
      "        ...,\n",
      "        [ 0.0142,  0.0210,  0.0028,  ..., -0.0022, -0.0075,  0.0117],\n",
      "        [ 0.0036, -0.0082, -0.0080,  ...,  0.0016,  0.0066,  0.0115],\n",
      "        [-0.0129,  0.0167, -0.0007,  ..., -0.0056,  0.0057,  0.0127]])\n",
      "tensor([[-0.0545, -0.3119,  0.0728,  ...,  0.0219, -0.2641, -0.1139],\n",
      "        [ 0.3051, -0.4491,  0.1069,  ..., -0.2051,  0.0627, -0.2501],\n",
      "        [-0.4285,  0.4684,  0.0269,  ...,  0.4216,  0.3620,  0.1785],\n",
      "        ...,\n",
      "        [ 0.6141,  0.5778, -0.2369,  ...,  0.3119, -0.1957,  0.1479],\n",
      "        [ 0.2356,  0.1989, -0.0877,  ..., -0.1797, -0.0481, -0.0640],\n",
      "        [-0.0164,  0.3942, -0.1560,  ...,  0.1617,  0.7052,  0.1235]])\n"
     ]
    }
   ],
   "source": [
    "print(initial_prompt_embeddings)\n",
    "print(final_prompt_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between initial and final prompt embeddings: tensor([12.2748, 11.9375, 12.1711, 13.1558, 12.3087, 12.7102, 12.3118, 15.2285,\n",
      "        12.1644, 12.3274, 13.8013, 11.8667, 13.0428, 12.7000, 11.8865])\n"
     ]
    }
   ],
   "source": [
    "embedding_difference = torch.norm(initial_prompt_embeddings - final_prompt_embeddings, dim=-1)\n",
    "\n",
    "# Print the differences for each virtual token\n",
    "print(\"Differences between initial and final prompt embeddings:\", embedding_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0:\n",
      "  Initial closest words: ['Class', ' Class', ' class', '_class', 'class']\n",
      "  Final closest words: [' SIP', ' praperadilan', ' ঘুর', 'gen', ' దల']\n",
      "Token 1:\n",
      "  Initial closest words: ['ify', 'ified', 'ifies', 'ifying', 'IFY']\n",
      "  Final closest words: [\">'\", \"_id'\", 'sprintf', ' sentència', ' sprintf']\n",
      "Token 2:\n",
      "  Initial closest words: [' part', ' parte', ' parts', 'part', 'Part']\n",
      "  Final closest words: [' parts', 'parts', ' part', ' Parts', '-part']\n",
      "Token 3:\n",
      "  Initial closest words: [' of', ',', ' de', ' and', '.']\n",
      "  Final closest words: [' of', ' của', ' स', ' पर', ' के']\n",
      "Token 4:\n",
      "  Initial closest words: [' speech', ' Speech', ' speeches', ' discurso', 'spe']\n",
      "  Final closest words: ['\\n                  ', '\\n          ', ' Bridge', ' साढ', ' ungg']\n",
      "Token 5:\n",
      "  Initial closest words: [' of', ',', ' de', ' and', '.']\n",
      "  Final closest words: [' من', ' أن', ' of', ' của', '于']\n",
      "Token 6:\n",
      "  Initial closest words: [' given', 'given', ' Given', ' dado', ' donné']\n",
      "  Final closest words: ['223', '>>', ' جهاز', ' 232', '>\"']\n",
      "Token 7:\n",
      "  Initial closest words: [' word', 'word', ' words', ' Word', ' palabra']\n",
      "  Final closest words: [' valves', '詞典', ' Etimologia:', ' valve', ' Subianto']\n",
      "Token 8:\n",
      "  Initial closest words: [' in', ' في', ',', ' of', ' to']\n",
      "  Final closest words: ['\\n         ', ',\\n       ', '.\\n       ', '\\n           ', ',\\n           ']\n",
      "Token 9:\n",
      "  Initial closest words: [' the', ' a', ',', '.', ' la']\n",
      "  Final closest words: [' ngành', ' inhibitors', 'ಗ್ರಾಮ', ' melan', ' datiboa']\n",
      "Token 10:\n",
      "  Initial closest words: [' sentence', ' sentences', ' phrase', '句子', ' Sent']\n",
      "  Final closest words: ['Work', '}{', ' ഉദ്യ', ' zoo', ' buy']\n",
      "Token 11:\n",
      "  Initial closest words: [' to', ',', ' and', ' of', '.']\n",
      "  Final closest words: ['angazo', 'ികം', '摩根', ' Wikinot', '华人']\n",
      "Token 12:\n",
      "  Initial closest words: [' noun', ' nouns', ' adjective', ' verbs', ' verb']\n",
      "  Final closest words: ['angular/core', ' ſ', ' chiết', ' datiboa', '形容词']\n",
      "Token 13:\n",
      "  Initial closest words: [',', '.', '،', '，', ' de']\n",
      "  Final closest words: ['</s>', ' là', '.\\n', '\">', ' é']\n",
      "Token 14:\n",
      "  Initial closest words: [' verb', ' Verb', ' verbs', ' verbal', 'Verb']\n",
      "  Final closest words: ['</s>', '\\n\\n\\n', ' ...\\n\\n', '...\\n\\n', '\\n  (']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def closest_words(embedding, tokenizer, model, top_k=5):\n",
    "    vocab_size = model.config.vocab_size\n",
    "    word_embeddings = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
    "    embedding = embedding.cpu().numpy()\n",
    "    similarities = cosine_similarity(embedding.reshape(1, -1), word_embeddings).flatten()\n",
    "    closest_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    closest_tokens = [tokenizer.decode([idx]) for idx in closest_indices]\n",
    "    return closest_tokens\n",
    "\n",
    "# Compare initial and final embeddings by finding the closest words\n",
    "for i, (initial_emb, final_emb) in enumerate(zip(initial_prompt_embeddings, final_prompt_embeddings)):\n",
    "    initial_closest_words = closest_words(initial_emb, tokenizer, model)\n",
    "    final_closest_words = closest_words(final_emb, tokenizer, model)\n",
    "    print(f\"Token {i}:\")\n",
    "    print(f\"  Initial closest words: {initial_closest_words}\")\n",
    "    print(f\"  Final closest words: {final_closest_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PeftModel, PeftConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# peft_model_id = \"./prompt_tuning/wn18rr/PT/template-4/llama3_chat\"\n",
    "\n",
    "# config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "# model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "device = \"cuda:1\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    f\"Perform a sentence completion on the following sentence: cover her face with a handkerchief. 'cover' part of speech is a ___. The answer is\",\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(inputs.input_ids)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10\n",
    "    )\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python3 prepare_prompt_tuning.py --kb_name=wn18rr --model_name=bloom_1b7 --template=template-3 --device=cuda --soft_prompt=PT --virtual_token=15 --train_size=0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1 python3 test.py --kb_name=geonames --model_name=bloom_1b7 --template=template-4 --device=cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python3 evaluator.py --kb_name=wn18rr --model=bloom_1b7 --template=template-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
